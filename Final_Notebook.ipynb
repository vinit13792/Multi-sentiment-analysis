{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SciLhCaxUfy",
        "outputId": "c0777f9f-53bd-45f8-91d8-55e7da89cf9c"
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-0.26.1-cp37-none-manylinux1_x86_64.whl (67.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 67.4 MB 53 kB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.26.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW0Mk9bl5Hhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "626f2994-f54f-4e39-9b0a-002538c8e69e"
      },
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "import catboost\n",
        "from collections import defaultdict, Counter\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "from prettytable import PrettyTable\n",
        "from tqdm.notebook import tqdm as tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import itertools\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from nltk.corpus import stopwords\n",
        "import pickle\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import joblib\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADQua80hydWk",
        "outputId": "d1d3adf3-a59f-4270-e76a-2fad86456555"
      },
      "source": [
        "!gdown --id '1aQ7Ns6DYhq3MKSoeQEG3gl7OLKgtRT44'\n",
        "!unzip 'greet models.zip' -d 'content/'\n",
        "\n",
        "!gdown --id '1Lz-bjuFshJY8LiXFgKpTZodqJqq6IYGX'\n",
        "!unzip 'backstory models.zip' -d 'content/'\n",
        "\n",
        "!gdown --id '1rja_wtez5BrfowiXIP91mveDAukMJTjH'\n",
        "!unzip 'justifn models.zip' -d 'content/'\n",
        "\n",
        "!gdown --id '1jyhHrzV4n26bamkj9Tlx-C-hI6SSy0DJ'\n",
        "!unzip 'Rant models.zip' -d 'content/'\n",
        "\n",
        "!gdown --id '19iQ0Weweam2gT5_9P8G6rry6U6fjYpSf'\n",
        "!unzip 'grat models.zip' -d 'content/'\n",
        "\n",
        "!gdown --id '1IVi2cGx66iXuyx-Q8UzdW55VtSf-E-ul'\n",
        "!unzip 'other models.zip' -d 'content/'\n",
        "\n",
        "!gdown --id '1pWLS_D8qPkbGHVL7TI0h_KNiVCjTKgdJ'\n",
        "!unzip 'expemo models.zip' -d 'content/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aQ7Ns6DYhq3MKSoeQEG3gl7OLKgtRT44\n",
            "To: /content/greet models.zip\n",
            "43.2MB [00:00, 49.7MB/s]\n",
            "Archive:  greet models.zip\n",
            "  inflating: content/greet models/calib_catd1_greet.sav  \n",
            "  inflating: content/greet models/calib_d1_greetrf.sav  \n",
            "  inflating: content/greet models/calib_d1xgb_greet.sav  \n",
            "  inflating: content/greet models/calib_dtc_greet.sav  \n",
            "  inflating: content/greet models/calib_meta_lrgreet.sav  \n",
            "  inflating: content/greet models/calib_sgdd1_greet.sav  \n",
            "  inflating: content/greet models/calib_svm_d1_greet.sav  \n",
            "  inflating: content/greet models/calib_svmrbf_d1_greet.sav  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Lz-bjuFshJY8LiXFgKpTZodqJqq6IYGX\n",
            "To: /content/backstory models.zip\n",
            "23.4MB [00:00, 64.2MB/s]\n",
            "Archive:  backstory models.zip\n",
            "  inflating: content/backstory models/calib_catd1_back.sav  \n",
            "  inflating: content/backstory models/calib_d1_backrf.sav  \n",
            "  inflating: content/backstory models/calib_d1xgb_back.sav  \n",
            "  inflating: content/backstory models/calib_dtc_back.sav  \n",
            "  inflating: content/backstory models/calib_meta2_lrback.sav  \n",
            "  inflating: content/backstory models/calib_sgdd1_back.sav  \n",
            "  inflating: content/backstory models/calib_svm_d1_back.sav  \n",
            "  inflating: content/backstory models/calib_svmrbf_d1_back.sav  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rja_wtez5BrfowiXIP91mveDAukMJTjH\n",
            "To: /content/justifn models.zip\n",
            "20.7MB [00:00, 65.6MB/s]\n",
            "Archive:  justifn models.zip\n",
            "  inflating: content/justifn models/calib_catd1_justifn.sav  \n",
            "  inflating: content/justifn models/calib_d1_justifnrf.sav  \n",
            "  inflating: content/justifn models/calib_d1xgb_justifn.sav  \n",
            "  inflating: content/justifn models/calib_dtc_justifn.sav  \n",
            "  inflating: content/justifn models/calib_meta2_lrjustifn.sav  \n",
            "  inflating: content/justifn models/calib_sgdd1_justifn.sav  \n",
            "  inflating: content/justifn models/calib_svm_d1_justifn.sav  \n",
            "  inflating: content/justifn models/calib_svmrbf_d1_justifn.sav  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jyhHrzV4n26bamkj9Tlx-C-hI6SSy0DJ\n",
            "To: /content/Rant models.zip\n",
            "14.8MB [00:00, 69.1MB/s]\n",
            "Archive:  Rant models.zip\n",
            "  inflating: content/Rant models/calib_catd1_rant.sav  \n",
            "  inflating: content/Rant models/calib_d1_rantrf.sav  \n",
            "  inflating: content/Rant models/calib_d1xgb_rant.sav  \n",
            "  inflating: content/Rant models/calib_dtc_rant.sav  \n",
            "  inflating: content/Rant models/calib_meta2_lrrant.sav  \n",
            "  inflating: content/Rant models/calib_sgdd1_rant.sav  \n",
            "  inflating: content/Rant models/calib_svm_d1_rant.sav  \n",
            "  inflating: content/Rant models/calib_svmrbf_d1_rant.sav  \n",
            "  inflating: content/Rant models/gcapi.dll  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19iQ0Weweam2gT5_9P8G6rry6U6fjYpSf\n",
            "To: /content/grat models.zip\n",
            "14.8MB [00:00, 88.0MB/s]\n",
            "Archive:  grat models.zip\n",
            "  inflating: content/grat models/calib_catd1_grat.sav  \n",
            "  inflating: content/grat models/calib_d1_gratrf.sav  \n",
            "  inflating: content/grat models/calib_d1xgb_grat.sav  \n",
            "  inflating: content/grat models/calib_dtc_grat.sav  \n",
            "  inflating: content/grat models/calib_meta2_lrgrat (1).sav  \n",
            "  inflating: content/grat models/calib_meta2_lrgrat.sav  \n",
            "  inflating: content/grat models/calib_sgdd1_grat.sav  \n",
            "  inflating: content/grat models/calib_svm_d1_grat.sav  \n",
            "  inflating: content/grat models/calib_svmrbf_d1_grat.sav  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IVi2cGx66iXuyx-Q8UzdW55VtSf-E-ul\n",
            "To: /content/other models.zip\n",
            "26.7MB [00:00, 73.1MB/s]\n",
            "Archive:  other models.zip\n",
            "  inflating: content/other models/calib_catd1_other.sav  \n",
            "  inflating: content/other models/calib_d1_otherrf.sav  \n",
            "  inflating: content/other models/calib_d1xgb_other.sav  \n",
            "  inflating: content/other models/calib_dtc_other.sav  \n",
            "  inflating: content/other models/calib_meta2_lrother.sav  \n",
            "  inflating: content/other models/calib_sgdd1_other.sav  \n",
            "  inflating: content/other models/calib_svm_d1_other.sav  \n",
            "  inflating: content/other models/calib_svmrbf_d1_other.sav  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pWLS_D8qPkbGHVL7TI0h_KNiVCjTKgdJ\n",
            "To: /content/expemo models.zip\n",
            "8.06MB [00:00, 49.1MB/s]\n",
            "Archive:  expemo models.zip\n",
            "  inflating: content/expemo models/calib_catd1_expemo.sav  \n",
            "  inflating: content/expemo models/calib_d1_expemorf.sav  \n",
            "  inflating: content/expemo models/calib_d1xgb_expemo.sav  \n",
            "  inflating: content/expemo models/calib_dtc_expemo.sav  \n",
            "  inflating: content/expemo models/calib_meta2_lrexpemo.sav  \n",
            "  inflating: content/expemo models/calib_sgdd1_expemo.sav  \n",
            "  inflating: content/expemo models/calib_svm_d1_expemo.sav  \n",
            "  inflating: content/expemo models/calib_svmrbf_d1_expemo.sav  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQZIzo1CV8Hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f321091-d5d9-495a-9685-9ab086a6909d"
      },
      "source": [
        "!gdown --id '1bY96HSWMuatJ8cprhWHVcTzUSK8farhh' # unigrams trained from training data\n",
        "!gdown --id '1nZk37wAd4BfUCNrLaquKpfsrXnWLG-Fu' # normaliser fitted on training data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bY96HSWMuatJ8cprhWHVcTzUSK8farhh\n",
            "To: /content/unigram_feat_multi.pkl\n",
            "100% 34.8k/34.8k [00:00<00:00, 60.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nZk37wAd4BfUCNrLaquKpfsrXnWLG-Fu\n",
            "To: /content/norm_trans.sav\n",
            "100% 129/129 [00:00<00:00, 229kB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnWQKDdvmpRt"
      },
      "source": [
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c-Fp3VgnDjI"
      },
      "source": [
        "def Find(string):\n",
        "  \n",
        "    # findall() has been used \n",
        "    # with valid conditions for urls in string\n",
        "    regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "    url = re.findall(regex,string) \n",
        "    temp = ''\n",
        "    for x in url:\n",
        "      temp+=''.join(x[0])\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AaFEb67m1c7"
      },
      "source": [
        "def clean_text(df, feature):\n",
        "    \n",
        "    cleaned_text = []\n",
        "    \n",
        "    for i in range(df.shape[0]):\n",
        "        \n",
        "        doc = df[feature].values[i]\n",
        "        \n",
        "        url = Find(doc)\n",
        "        \n",
        "        doc = re.sub(url, '', doc)\n",
        "        \n",
        "        doc = re.findall(r'\\w+', doc)\n",
        "        \n",
        "        table = str.maketrans('', '', string.punctuation)\n",
        "        \n",
        "        stripped = [w.translate(table) for w in doc]\n",
        "        \n",
        "        doc = ' '.join(stripped)\n",
        "        \n",
        "        doc = doc.lower()\n",
        "\n",
        "        # remove text followed by numbers\n",
        "        doc = re.sub('[^A-Za-z0-9]+', ' ', doc)\n",
        "\n",
        "        # remove text which appears inside < > or text preceeding or suceeding <, >\n",
        "        doc = re.sub(r'< >|<.*?>|<>|\\>|\\<', ' ', doc)\n",
        "\n",
        "        # remove anything inside brackets\n",
        "        doc = re.sub(r'\\(.*?\\)', ' ', doc)\n",
        "        \n",
        "        # remove digits\n",
        "        doc = re.sub(r'\\d+', ' ', doc)\n",
        "        cleaned_text.append(doc)\n",
        "        \n",
        "    return cleaned_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQZoAcQEa9ve"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u_vbEnOn-sZ"
      },
      "source": [
        "def get_word_count(data, feature):\n",
        "    \n",
        "    counts = []\n",
        "    \n",
        "    for i in range(data[feature].shape[0]):\n",
        "        \n",
        "        text = data[feature].values[i]\n",
        "        pattern = r'[a-zA-Z]+'\n",
        "        \n",
        "        words = re.findall(pattern, text)\n",
        "        \n",
        "        counts.append(len(words))\n",
        "        \n",
        "    return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C09ix38oLlj"
      },
      "source": [
        "def pos_count(data, feature):\n",
        "\n",
        "  POS_List = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'PRP', 'PRPS', 'RB', 'RBR', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP','VBZ', 'WP', 'WP$']\n",
        "\n",
        "  pos_per_text = defaultdict(list)\n",
        "\n",
        "  for i in range(data.shape[0]):\n",
        "\n",
        "    doc = data['Text'].values[i]\n",
        "\n",
        "    info =[]\n",
        "    tokenized = sent_tokenize(doc)\n",
        "    for i in tokenized:\n",
        "\n",
        "      # Word tokenizers is used to find the words \n",
        "      # and punctuation in a string\n",
        "      wordsList = nltk.word_tokenize(i)\n",
        "  \n",
        "      # removing stop words from wordList\n",
        "      wordsList = [w for w in wordsList if not w in stop_words] \n",
        "  \n",
        "      #  Using a Tagger. Which is part-of-speech \n",
        "      # tagger or POS-tagger. \n",
        "      tagged = nltk.pos_tag(wordsList)\n",
        "    info=[tag[1] for tag in tagged]\n",
        "    #print(info)\n",
        "    \n",
        "    \n",
        "    counts = Counter(info)\n",
        "    \n",
        "    keys = dict(counts).keys()\n",
        "    #print(list(keys))\n",
        "    #break\n",
        "    for pos in POS_List:\n",
        "      \n",
        "      if pos in list(keys):\n",
        "        \n",
        "        pos_per_text[pos].append(counts.get(pos))\n",
        "            \n",
        "      else:\n",
        "        pos_per_text[pos].append(0)\n",
        "      \n",
        "  return pos_per_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvyEwiwdoO0R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAGXhBUpo2XU"
      },
      "source": [
        "def check_negation(df):\n",
        "    \n",
        "    negex_ = []\n",
        "    \n",
        "    for i in range(df.shape[0]):\n",
        "        sent = df['Text'].values[i].split()\n",
        "        if ('not' in sent) or ('never' in sent):\n",
        "            negex_.append(1)\n",
        "        else:\n",
        "            negex_.append(0)\n",
        "    return negex_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PGK6kpZR0u0"
      },
      "source": [
        "def get_pos_vec(df, feature):\n",
        "  ci = ['CC', 'DT', 'EX', 'IN', 'MD', 'PDT', 'POS', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'WDT', 'WP', 'WP$', 'WRB']\n",
        "  gfi = ['CD', 'FW', 'LS', 'NNP', 'NNPS', 'PRP', 'PRP$', 'SYM', 'UH']\n",
        "  ei = ['JJ', 'JJR','JJS','NN','NNS','VB','VBD','VBD','VBG','VBN','VBP','VBZ']\n",
        "  pos_cat = defaultdict(list)\n",
        "  \n",
        "  for i in range(df.shape[0]):\n",
        "\n",
        "    doc = df[feature].values[i]\n",
        "    info =[]\n",
        "    tokenized = sent_tokenize(doc)\n",
        "    for i in tokenized:\n",
        "\n",
        "      # Word tokenizers is used to find the words \n",
        "      # and punctuation in a string\n",
        "      wordsList = nltk.word_tokenize(i)\n",
        "  \n",
        "      # removing stop words from wordList\n",
        "      wordsList = [w for w in wordsList if not w in stop_words] \n",
        "  \n",
        "      #  Using a Tagger. Which is part-of-speech \n",
        "      # tagger or POS-tagger. \n",
        "      tagged = nltk.pos_tag(wordsList)\n",
        "\n",
        "    pattern = [tag[1] for tag in tagged]\n",
        "    \n",
        "    \n",
        "    pat_check_ci = [pat for pat in pattern if pat in ci]\n",
        "    pat_check_ei = [pat for pat in pattern if pat in ei]\n",
        "    pat_check_gfi = [pat for pat in pattern if pat in gfi]\n",
        "    #print('-----')\n",
        "    #print(pat_check_ci, 'ci')\n",
        "    #print(pat_check_gfi, 'gfi')\n",
        "    #print(pat_check_ei, 'ei')\n",
        "    \n",
        "    if (len(pat_check_ci)!=0) and (len(pat_check_ei)!=0) and (len(pat_check_gfi)!=0):\n",
        "      pos_cat['CI'].append(1)\n",
        "      pos_cat['EI'].append(1)\n",
        "      pos_cat['GFI'].append(1)\n",
        "    elif (len(pat_check_ci)==0) and (len(pat_check_ei)!=0) and (len(pat_check_gfi)!=0):\n",
        "      pos_cat['CI'].append(0)\n",
        "      pos_cat['EI'].append(1)\n",
        "      pos_cat['GFI'].append(1)\n",
        "    elif (len(pat_check_ci)!=0) and (len(pat_check_ei)==0) and (len(pat_check_gfi)!=0):\n",
        "      pos_cat['CI'].append(1)\n",
        "      pos_cat['EI'].append(0)\n",
        "      pos_cat['GFI'].append(1)\n",
        "    elif (len(pat_check_ci)!=0) and (len(pat_check_ei)!=0) and (len(pat_check_gfi)==0):\n",
        "      pos_cat['CI'].append(1)\n",
        "      pos_cat['EI'].append(1)\n",
        "      pos_cat['GFI'].append(0)\n",
        "    elif (len(pat_check_ci)!=0) and (len(pat_check_ei)==0) and (len(pat_check_gfi)==0):\n",
        "      pos_cat['CI'].append(1)\n",
        "      pos_cat['EI'].append(0)\n",
        "      pos_cat['GFI'].append(0)\n",
        "    elif (len(pat_check_ci)==0) and (len(pat_check_ei)==0) and (len(pat_check_gfi)!=0):\n",
        "      pos_cat['CI'].append(0)\n",
        "      pos_cat['EI'].append(0)\n",
        "      pos_cat['GFI'].append(1)\n",
        "    else:\n",
        "      pos_cat['CI'].append(0)\n",
        "      pos_cat['EI'].append(1)\n",
        "      pos_cat['GFI'].append(0)\n",
        "\n",
        "  return pos_cat\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IhcesYig_qu"
      },
      "source": [
        "\n",
        "unigram_feat_multi = pickle.load(open('unigram_feat_multi.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbG9YAL_5EMI"
      },
      "source": [
        "def text_to_seq(vocab, data):\n",
        "\n",
        "  sequences = []\n",
        "  for i in range(data.shape[0]):\n",
        "    text = re.findall(r'[a-zA-Z]+|\\~|\\!|\\@|\\#|\\$|\\%|\\^|\\&|\\*|\\(|\\)|\\-\\+|\\,|\\\"|\\?|\\.|\\:|\\;|\\=|\\[|\\]|\\{|\\}|\\_|\\\\|\\/', data['Text'].values[i])\n",
        "    seq = []\n",
        "    for i in text:\n",
        "      if vocab.get(i)==None:\n",
        "        seq.append(1)\n",
        "      else:\n",
        "        seq.append(vocab.get(i))\n",
        "    sequences.append(seq)\n",
        "\n",
        "  return np.array(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QsS8YEt5Omi"
      },
      "source": [
        "normaliser = joblib.load('norm_trans.sav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st0b6imVCEFI"
      },
      "source": [
        "def predict(X):\n",
        "\n",
        "  greet_cat = joblib.load('content/greet models/calib_catd1_greet.sav')\n",
        "  greet_rf = joblib.load('content/greet models/calib_d1_greetrf.sav')\n",
        "  greet_xgb = joblib.load('content/greet models/calib_d1xgb_greet.sav')\n",
        "  greet_dtc = joblib.load('content/greet models/calib_dtc_greet.sav')\n",
        "  greet_sgd = joblib.load('content/greet models/calib_sgdd1_greet.sav')\n",
        "  greet_svm = joblib.load('content/greet models/calib_svm_d1_greet.sav')\n",
        "  greet_svmrbf = joblib.load('content/greet models/calib_svmrbf_d1_greet.sav')\n",
        "  greet_meta = joblib.load('content/greet models/calib_meta_lrgreet.sav')\n",
        "\n",
        "  greet_cat_pred = greet_cat.predict_proba(X)[:,1]\n",
        "  greet_rf_pred = greet_rf.predict_proba(X)[:,1]\n",
        "  greet_xgb_pred = greet_xgb.predict_proba(X)[:,1]\n",
        "  greet_dtc_pred = greet_dtc.predict_proba(X)[:,1]\n",
        "  greet_sgd_pred = greet_sgd.predict_proba(X)[:,1]\n",
        "  greet_svm_pred = greet_svm.predict_proba(X)[:,1]\n",
        "  greet_svmrbf_pred = greet_svmrbf.predict_proba(X)[:,1]\n",
        "\n",
        "  greet_meta_array = np.hstack((greet_cat_pred.reshape(-1,1), greet_rf_pred.reshape(-1,1), greet_xgb_pred.reshape(-1,1),\n",
        "                                greet_dtc_pred.reshape(-1,1), greet_sgd_pred.reshape(-1,1), \n",
        "                          greet_svm_pred.reshape(-1,1), greet_svmrbf_pred.reshape(-1,1)))\n",
        "  \n",
        "  greet_meta_preds = np.round(greet_meta.predict_proba(greet_meta_array)[:,1],2)\n",
        "\n",
        "  back_cat = joblib.load('content/backstory models/calib_catd1_back.sav')\n",
        "  back_rf = joblib.load('content/backstory models/calib_d1_backrf.sav')\n",
        "  back_xgb = joblib.load('content/backstory models/calib_d1xgb_back.sav')\n",
        "  back_dtc = joblib.load('content/backstory models/calib_dtc_back.sav')\n",
        "  back_sgd = joblib.load('content/backstory models/calib_sgdd1_back.sav')\n",
        "  back_svm = joblib.load('content/backstory models/calib_svm_d1_back.sav')\n",
        "  back_svmrbf = joblib.load('content/backstory models/calib_svmrbf_d1_back.sav')\n",
        "  back_meta = joblib.load('content/backstory models/calib_meta2_lrback.sav')\n",
        "\n",
        "  back_cat_pred = back_cat.predict_proba(X)[:,1]\n",
        "  back_rf_pred = back_rf.predict_proba(X)[:,1]\n",
        "  back_xgb_pred = back_xgb.predict_proba(X)[:,1]\n",
        "  back_dtc_pred = back_dtc.predict_proba(X)[:,1]\n",
        "  back_sgd_pred = back_sgd.predict_proba(X)[:,1]\n",
        "  back_svm_pred = back_svm.predict_proba(X)[:,1]\n",
        "  back_svmrbf_pred = back_svmrbf.predict_proba(X)[:,1]\n",
        "\n",
        "  back_meta_array = np.hstack((back_cat_pred.reshape(-1,1), back_rf_pred.reshape(-1,1), back_xgb_pred.reshape(-1,1),\n",
        "                               back_dtc_pred.reshape(-1,1), back_sgd_pred.reshape(-1,1), \n",
        "                          back_svm_pred.reshape(-1,1), back_svmrbf_pred.reshape(-1,1)))\n",
        "  \n",
        "  back_meta_preds = np.round(back_meta.predict_proba(back_meta_array)[:,1],2)\n",
        "\n",
        "  justifn_cat = joblib.load('content/justifn models/calib_catd1_justifn.sav')\n",
        "  justifn_rf = joblib.load('content/justifn models/calib_d1_justifnrf.sav')\n",
        "  justifn_xgb = joblib.load('content/justifn models/calib_d1xgb_justifn.sav')\n",
        "  justifn_dtc = joblib.load('content/justifn models/calib_dtc_justifn.sav')\n",
        "  justifn_sgd = joblib.load('content/justifn models/calib_sgdd1_justifn.sav')\n",
        "  justifn_svm = joblib.load('content/justifn models/calib_svm_d1_justifn.sav')\n",
        "  justifn_svmrbf = joblib.load('content/justifn models/calib_svmrbf_d1_justifn.sav')\n",
        "  justifn_meta = joblib.load('content/justifn models/calib_meta2_lrjustifn.sav')\n",
        "\n",
        "  justifn_cat_pred = justifn_cat.predict_proba(X)[:,1]\n",
        "  justifn_rf_pred = justifn_rf.predict_proba(X)[:,1]\n",
        "  justifn_xgb_pred = justifn_xgb.predict_proba(X)[:,1]\n",
        "  justifn_dtc_pred = justifn_dtc.predict_proba(X)[:,1]\n",
        "  justifn_sgd_pred = justifn_sgd.predict_proba(X)[:,1]\n",
        "  justifn_svm_pred = justifn_svm.predict_proba(X)[:,1]\n",
        "  justifn_svmrbf_pred = justifn_svmrbf.predict_proba(X)[:,1]\n",
        "\n",
        "  justifn_meta_array = np.hstack((justifn_cat_pred.reshape(-1,1), justifn_rf_pred.reshape(-1,1), justifn_xgb_pred.reshape(-1,1),\n",
        "                                  justifn_dtc_pred.reshape(-1,1), justifn_sgd_pred.reshape(-1,1), \n",
        "                          justifn_svm_pred.reshape(-1,1), justifn_svmrbf_pred.reshape(-1,1)))\n",
        "  \n",
        "  justifn_meta_preds = np.round(justifn_meta.predict_proba(justifn_meta_array)[:,1],2)\n",
        "\n",
        "  \n",
        "  rant_cat = joblib.load('content/Rant models/calib_catd1_rant.sav')\n",
        "  rant_rf = joblib.load('content/Rant models/calib_d1_rantrf.sav')\n",
        "  rant_xgb = joblib.load('content/Rant models/calib_d1xgb_rant.sav')\n",
        "  rant_dtc = joblib.load('content/Rant models/calib_dtc_rant.sav')\n",
        "  rant_sgd = joblib.load('content/Rant models/calib_sgdd1_rant.sav')\n",
        "  rant_svm = joblib.load('content/Rant models/calib_svm_d1_rant.sav')\n",
        "  rant_svmrbf = joblib.load('content/Rant models/calib_svmrbf_d1_rant.sav')\n",
        "  rant_meta = joblib.load('content/Rant models/calib_meta2_lrrant.sav')\n",
        "\n",
        "  rant_cat_pred = rant_cat.predict_proba(X)[:,1]\n",
        "  rant_rf_pred = rant_rf.predict_proba(X)[:,1]\n",
        "  rant_xgb_pred = rant_xgb.predict_proba(X)[:,1]\n",
        "  rant_dtc_pred = rant_dtc.predict_proba(X)[:,1]\n",
        "  rant_sgd_pred = rant_sgd.predict_proba(X)[:,1]\n",
        "  rant_svm_pred = rant_svm.predict_proba(X)[:,1]\n",
        "  rant_svmrbf_pred = rant_svmrbf.predict_proba(X)[:,1]\n",
        "\n",
        "  rant_meta_array = np.hstack((rant_cat_pred.reshape(-1,1), rant_rf_pred.reshape(-1,1), rant_xgb_pred.reshape(-1,1),\n",
        "                               rant_dtc_pred.reshape(-1,1), rant_sgd_pred.reshape(-1,1), \n",
        "                          rant_svm_pred.reshape(-1,1), rant_svmrbf_pred.reshape(-1,1)))\n",
        "  \n",
        "  rant_meta_preds = np.round(rant_meta.predict_proba(rant_meta_array)[:,1],2)\n",
        "\n",
        "  \n",
        "  other_cat = joblib.load('content/other models/calib_catd1_other.sav')\n",
        "  other_rf = joblib.load('content/other models/calib_d1_otherrf.sav')\n",
        "  other_xgb = joblib.load('content/other models/calib_d1xgb_other.sav')\n",
        "  other_dtc = joblib.load('content/other models/calib_dtc_other.sav')\n",
        "  other_sgd = joblib.load('content/other models/calib_sgdd1_other.sav')\n",
        "  other_svm = joblib.load('content/other models/calib_svm_d1_other.sav')\n",
        "  other_svmrbf = joblib.load('content/other models/calib_svmrbf_d1_other.sav')\n",
        "  other_meta = joblib.load('content/other models/calib_meta2_lrother.sav')\n",
        "\n",
        "  other_cat_pred = other_cat.predict_proba(X)[:,1]\n",
        "  other_rf_pred = other_rf.predict_proba(X)[:,1]\n",
        "  other_xgb_pred = other_xgb.predict_proba(X)[:,1]\n",
        "  other_dtc_pred = other_dtc.predict_proba(X)[:,1]\n",
        "  other_sgd_pred = other_sgd.predict_proba(X)[:,1]\n",
        "  other_svm_pred = other_svm.predict_proba(X)[:,1]\n",
        "  other_svmrbf_pred = other_svmrbf.predict_proba(X)[:,1]\n",
        "\n",
        "  other_meta_array = np.hstack((other_cat_pred.reshape(-1,1), other_rf_pred.reshape(-1,1), other_xgb_pred.reshape(-1,1),\n",
        "                                other_dtc_pred.reshape(-1,1), other_sgd_pred.reshape(-1,1), \n",
        "                          other_svm_pred.reshape(-1,1), other_svmrbf_pred.reshape(-1,1)))\n",
        "  \n",
        "  other_meta_preds = np.round(other_meta.predict_proba(other_meta_array)[:,1],2)\n",
        "\n",
        "  \n",
        "  expemo_cat = joblib.load('content/expemo models/calib_catd1_expemo.sav')\n",
        "  expemo_rf = joblib.load('content/expemo models/calib_d1_expemorf.sav')\n",
        "  expemo_xgb = joblib.load('content/expemo models/calib_d1xgb_expemo.sav')\n",
        "  expemo_dtc = joblib.load('content/expemo models/calib_dtc_expemo.sav')\n",
        "  expemo_sgd = joblib.load('content/expemo models/calib_sgdd1_expemo.sav')\n",
        "  expemo_svm = joblib.load('content/expemo models/calib_svm_d1_expemo.sav')\n",
        "  expemo_svmrbf = joblib.load('content/expemo models/calib_svmrbf_d1_expemo.sav')\n",
        "  expemo_meta = joblib.load('content/expemo models/calib_meta2_lrexpemo.sav')\n",
        "\n",
        "  expemo_cat_pred = expemo_cat.predict_proba(X)[:,1]\n",
        "  expemo_rf_pred = expemo_rf.predict_proba(X)[:,1]\n",
        "  expemo_xgb_pred = expemo_xgb.predict_proba(X)[:,1]\n",
        "  expemo_dtc_pred = expemo_dtc.predict_proba(X)[:,1]\n",
        "  expemo_sgd_pred = expemo_sgd.predict_proba(X)[:,1]\n",
        "  expemo_svm_pred = expemo_svm.predict_proba(X)[:,1]\n",
        "  expemo_svmrbf_pred = expemo_svmrbf.predict_proba(X)[:,1]\n",
        "\n",
        "  expemo_meta_array = np.hstack((expemo_cat_pred.reshape(-1,1), expemo_rf_pred.reshape(-1,1), expemo_xgb_pred.reshape(-1,1),\n",
        "                                 expemo_dtc_pred.reshape(-1,1), expemo_sgd_pred.reshape(-1,1), \n",
        "                          expemo_svm_pred.reshape(-1,1), expemo_svmrbf_pred.reshape(-1,1)))\n",
        "  \n",
        "  expemo_meta_preds = np.round(expemo_meta.predict_proba(expemo_meta_array)[:,1],2)\n",
        "\n",
        "  x = PrettyTable()\n",
        "  x.field_names = ['Greeting','Backstory','Justification','Rant','Other','Express Emotion']\n",
        "  x.add_row([greet_meta_preds, back_meta_preds, justifn_meta_preds, rant_meta_preds, other_meta_preds, expemo_meta_preds])\n",
        "  print(x)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvnxXfUUmUqI"
      },
      "source": [
        "def preprocess(X):\n",
        "\n",
        "  POS_List = ['JJ', 'JJR', 'JJS', 'NN', 'NNS', 'PRP', 'PRPS', 'RB', 'RBR', 'RP', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP','VBZ', 'WP', 'WP$']\n",
        "\n",
        "  df = pd.DataFrame(data={X}, columns=['Text'])\n",
        "  df['Text'].values[0] = decontracted(df['Text'].values[0])\n",
        "  df['Clean Text'] = clean_text(df, 'Text')\n",
        "  df['word_count'] = get_word_count(df, 'Text')\n",
        "  parts_of_speech_counts = pos_count(df, 'Text')\n",
        "\n",
        "  for pos in POS_List:\n",
        "    df[pos] = parts_of_speech_counts.get(pos)\n",
        "\n",
        "  df['Negation'] = check_negation(df)\n",
        "\n",
        "  pos_cat = get_pos_vec(df, 'Text')\n",
        "  df['CI'] = pos_cat.get('CI')\n",
        "  df['GFI'] = pos_cat.get('GFI')\n",
        "  df['EI'] = pos_cat.get('EI')\n",
        "  \n",
        "  unigram_feat_multi = pickle.load(open('unigram_feat_multi.pkl', 'rb'))\n",
        "  puncs = [i for i in string.punctuation]\n",
        "  unigram_feat_multi = puncs + list(unigram_feat_multi)\n",
        "  dictionary_multi = list(unigram_feat_multi)\n",
        "\n",
        "  word_index_multi= dict()\n",
        "\n",
        "  for i in range(len(dictionary_multi)):\n",
        "      if i==0:\n",
        "        word_index_multi['OOV'] = 1\n",
        "      else:\n",
        "        word_index_multi[dictionary_multi[i]]=i+1\n",
        "  # Based on max length of sentences in training data\n",
        "  maxlen = 265\n",
        "\n",
        "  text_array = text_to_seq(word_index_multi, df)\n",
        "  text_array = pad_sequences(text_array, maxlen=maxlen, dtype='int32', padding='pre',truncating='post')\n",
        "  df.drop('Clean Text', axis=1, inplace=True)\n",
        "  keep_columns = list(df.columns[1:])\n",
        "  data_cols = df[keep_columns].values\n",
        "\n",
        "  query = np.hstack((text_array, data_cols))\n",
        "  query = normaliser.transform(query)\n",
        "  \n",
        "  predict(query)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KijDwgOFyGPH",
        "outputId": "429709cc-3010-4936-92ec-99f8826e93dc"
      },
      "source": [
        "X = str(input())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tripadvisor staff removed this post because it did not meet Tripadvisor's forum posting guidelines with prohibiting self-promotional advertising or solicitation.  We ask all of our members to keep their forum messages free of self-promoting advertisements or solicitation of any kind - members affiliated with any tourism-related business should not include commercial contact information or URLs in their forum messages.  To review the Tripadvisor Forums Posting Guidelines, please follow this link: http://www.tripadvisor.com/pages/forums_posting_guidelines.html  We remove posts that do not follow our posting guidelines, and we reserve the right to remove any post for any reason.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuZ7PHHX_pXI",
        "outputId": "c5c82045-a4ba-4408-fff5-de0e88fc6802"
      },
      "source": [
        "preprocess(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----------+---------------+--------+--------+-----------------+\n",
            "| Greeting | Backstory | Justification |  Rant  | Other  | Express Emotion |\n",
            "+----------+-----------+---------------+--------+--------+-----------------+\n",
            "|  [0.31]  |   [0.75]  |     [0.02]    | [0.04] | [0.32] |      [0.02]     |\n",
            "+----------+-----------+---------------+--------+--------+-----------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyfMlwRrC1gt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}